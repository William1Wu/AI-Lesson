(1) CNN 常用參數設置
　<1> Learning Rate：反向傳播網路中更新 weighting 矩陣的步長，數值落在 0∼1。
　<2> Batch Size：一次流入模型的 dataset 樣本數，數值落在 1∼N。
　　　迭代次數相同時，batch size 越大，泛化能力越強，模型越穩定，loss 曲線越平滑，模型也更快收斂，但每次迭代會花更多時間。
　<3> Epoch：指定所有訓練 dataset 要在模型中訓練幾輪，數值落在 1∼N。
　　　當模型較為簡單或訓練 dataset 規模較小時，epoch 不宜過高，否則模型容易 overfitting，
　　　而當模型較為複雜或訓練 dataset 規模夠大時，則可適當提高 epoch 數。
　<4> Weighting Decay：模型訓練過程中反向傳播 weighting 更新的衰減率，數值常落在 0∼0.001。

(2) 如何提高 CNN 泛化能力，也就是不要 overfitting
　<1> 增加訓練 dataset。
　<2> 用更大的 batch size 訓練。
　<3> 調整訓練 dataset 的分布，使其均勻。
　<4> 調整目標函數。
　<5> 調整網路結構。淺層卷積神經網路的參數量較少，使模型的泛化能力不足而導致欠擬合，此時若疊加卷積層可有效增加網路參數，提高模型表達能力。
　　　而深層卷積網路若沒有充足的訓練 dataset 則容易導致模型過擬合，此時若減少卷積層數，則可提高模型泛化能力。
　<6> 做數據增強。在有限數據的前提下通過平移、旋轉、加 noise 等一些變化來增加訓練數據，提高模型泛化能力。注意數據變化應盡可能不破壞原數據的主體特徵。
　<7> weighting 正則化。一般是在 loss function 中添加一項 weighting 矩陣的正則項作為懲罰項，用來懲罰 loss 較小時網路 weighting 過大的情況（此時常常是 weighting 對數據樣本 overfitting）。
　<8> 屏蔽網路節點。可理解為網路結構的正則化，透過隨機屏蔽某些神經元的輸出，可使模型的容錯性更強。
